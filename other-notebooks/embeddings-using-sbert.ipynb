{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23fzojQN3nBg",
        "outputId": "2b6429b9-05c5-4646-e62d-57a4feb8fd05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (1.10.7)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nepalitokenizer\n",
            "  Downloading nepalitokenizer-1.8.6.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: snowballstemmer in /usr/local/lib/python3.9/dist-packages (2.2.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.1/492.1 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn) (8.1.3)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.9/dist-packages (from gunicorn) (67.7.2)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic) (4.5.0)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hdbscan>=0.8.29\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (5.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.5.3)\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (4.65.0)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (6.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.34)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.1+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi) (3.6.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.11.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: fasttext, pyngrok, hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395605 sha256=a70aa1edd2648861d02507dee47a64c9a77ef5993a00458f3cfb2a234f370e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19879 sha256=08a148e7b0c87071dc0228caa56e70bddaa066ad73dec1d12dcb0699d01a19bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/49/9c/44b13823eb256a3b4dff34b972f7a3c7d9910bfef269e59bd7\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=3580419 sha256=d1500cf22e88c01dadf4b1a52c5e631b7644e2464755c0df88ddd82eba7eb122\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=feb92fd0ddc0fb68ea26b73cd12d5f42c46f86b2dd002f1b2b9c57f0d66f963e\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=e25ad88431ea683c1618795b75a1d16ef332ce41fb70414b6a3f35de1013dd2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55640 sha256=6211d32a3a7a2f570ddfda6469f957283665dcc104aea093dc06abdaa5325485\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/f9/4d/ec5ad1c823c710fcc4473669fdcffc8891f4bc398c841af22e\n",
            "Successfully built fasttext pyngrok hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, nepalitokenizer, pyngrok, pybind11, h11, gunicorn, dnspython, uvicorn, starlette, pymongo, huggingface-hub, fasttext, transformers, pynndescent, hdbscan, fastapi, umap-learn, sentence-transformers, bertopic\n",
            "Successfully installed bertopic-0.14.1 dnspython-2.3.0 fastapi-0.95.1 fasttext-0.9.2 gunicorn-20.1.0 h11-0.14.0 hdbscan-0.8.29 huggingface-hub-0.14.1 nepalitokenizer-1.8.6.0 pybind11-2.10.4 pymongo-4.3.3 pyngrok-6.0.0 pynndescent-0.5.10 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.26.1 tokenizers-0.13.3 transformers-4.28.1 umap-learn-0.5.3 uvicorn-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install uvicorn gunicorn fastapi pydantic bertopic fasttext nepalitokenizer snowballstemmer pyngrok pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgStfJpc4N4w",
        "outputId": "29609c40-9f21-4717-cdf4-1951b0bf25bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRJ-6Ks_4RNW",
        "outputId": "c42901f1-ebda-4a38-b096-c71c3578a918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive for fasttext file\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "942iFxz84Uxt",
        "outputId": "53c7ce47-cf17-4735-89fe-10686e9bd559"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import fasttext\n",
        "import numpy as np\n",
        "from bertopic.backend import BaseEmbedder\n",
        "from bertopic import BERTopic\n",
        "from nepalitokenizer import NepaliTokenizer\n",
        "import snowballstemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import requests\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "from fastapi.responses import JSONResponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRM1lNt_4YV4",
        "outputId": "a6d66112-6182-499d-f853-35cbe3a76f5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "app= FastAPI()\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins = origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"]\n",
        ")\n",
        "\n",
        "#MongoDB and scrapy cloud api\n",
        "myclient = pymongo.MongoClient(\"mongodburl\")\n",
        "mydb = myclient[\"major-project\"]\n",
        "\n",
        "ok_url = \"https://app.zyte.com/api/items.json?project=649148&spider=ok_latest&include_headers=1&apikey=5838ef376c21473bb230876352380793\"\n",
        "\n",
        "ap_url = \"https://app.zyte.com/api/items.json?project=649148&spider=ap_latest&include_headers=1&apikey=5838ef376c21473bb230876352380793\"\n",
        "\n",
        "mycol = mydb[\"news_np_lb_pred\"]\n",
        "\n",
        "#Preprocessing the fetched news titles\n",
        "stopword= stopwords.words('nepali')\n",
        "stemmer = snowballstemmer.stemmer('nepali')\n",
        "\n",
        "def nepali_tokenizer(text):\n",
        "    # tokenize the text using the BERT tokenizer\n",
        "    tokens = tokenize.tokenizer(text)\n",
        "    # return the token list as a string\n",
        "    return tokens\n",
        "\n",
        "tokenize = NepaliTokenizer()\n",
        "\n",
        "## Creating custom embedder using fasttext\n",
        "class CustomEmbedder(BaseEmbedder):\n",
        "    def __init__(self, embedding_model):\n",
        "        super().__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def embed(self, documents, verbose=False):\n",
        "        results_from_fasttext = []\n",
        "        for sentence in documents:\n",
        "            embeddings_fasttext = self.embedding_model.get_sentence_vector(sentence).tolist()\n",
        "            embeddings_fasttext = np.asarray(embeddings_fasttext).reshape(-1,300).flatten()\n",
        "            results_from_fasttext.append(embeddings_fasttext)\n",
        "\n",
        "        embeddings= np.array(results_from_fasttext)\n",
        "        return embeddings\n",
        "\n",
        "# Create custom backend\n",
        "ft_ne = fasttext.load_model(\"/content/gdrive/MyDrive/major-project/cc.ne.300.bin\")\n",
        "custom_embedder = CustomEmbedder(embedding_model=ft_ne)\n",
        "\n",
        "# Importing Trained Bertopic Model\n",
        "topic_model = BERTopic.load(\"/content/gdrive/MyDrive/major-project/pro_first_2k_data\")\n",
        "\n",
        "\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "class NewsItem(BaseModel):\n",
        "    documents: List[str]\n",
        "\n",
        "\n",
        "\n",
        "@app.post('/predict')\n",
        "async def predict_endpoint(item:NewsItem):\n",
        "    # print(item.documents)\n",
        "    # print(type(item.documents))\n",
        "    item_preprocess = list(map(tokenize.tokenizer, item.documents))\n",
        "    item_preprocess = list(map(stemmer.stemWords,item_preprocess))\n",
        "    item_preprocess = list(map(lambda x: \" \".join([w for w in x if w not in stopword]),item_preprocess))\n",
        "    # print(item_preprocess)\n",
        "\n",
        "    # Creating embeddings for new topics\n",
        "    embeddings_steemed = custom_embedder.embed(item_preprocess)\n",
        "    # Predict topics for test_docs\n",
        "    predicted_topics, predicted_probs = topic_model.transform(item_preprocess,embeddings_steemed)\n",
        "    print(predicted_topics)\n",
        "\n",
        "    # predicted_topics = list(map(int, predicted_topics))\n",
        "    names = []\n",
        "    for label in predicted_topics:\n",
        "      name = topic_info[topic_info[\"Topic\"]==label][\"Name\"].to_string(index=False)\n",
        "      names.append(name)\n",
        "\n",
        "    return dict(zip(item.documents,names))\n",
        "\n",
        "@app.get('/predictandpost')\n",
        "async def predict_post():\n",
        "  response_ok=requests.get(ok_url)\n",
        "  response_ap=requests.get(ap_url)\n",
        "\n",
        "\n",
        "  news_text_df = pd.read_json(response_ap.text,orient=\"records\")\n",
        "\n",
        "  ok_df = pd.read_json(response_ok.text,orient=\"records\")\n",
        "\n",
        "  news_text_df = pd.concat([news_text_df,ok_df],ignore_index=True)\n",
        "\n",
        "  mydoc = mycol.find({\"_type\":\"NewscrawlerItem\"},{\"link\":1,\"_id\":0})\n",
        "\n",
        "  news_text_df_old = pd.DataFrame(list(mydoc))\n",
        "\n",
        "  news_text_df[\"headline_stemmed\"] = news_text_df[\"headline\"].apply(tokenize.tokenizer)\n",
        "  news_text_df[\"headline_stemmed\"] = news_text_df[\"headline_stemmed\"].apply(stemmer.stemWords)\n",
        "  news_text_df[\"headline_stemmed\"]= news_text_df[\"headline_stemmed\"].apply(lambda x: \" \".join([w for w in x if w not in stopword]))\n",
        "\n",
        "  # Creating embeddings for new topics\n",
        "  embeddings_steemed = custom_embedder.embed(news_text_df[\"headline_stemmed\"])\n",
        "  # Predict topics for test_docs\n",
        "  predicted_topics, predicted_probs = topic_model.transform(news_text_df[\"headline_stemmed\"],embeddings_steemed)\n",
        "\n",
        "  news_text_df[\"topic_label\"] = predicted_topics\n",
        "\n",
        "  ## SAVING LABEL NAME TO DATABASE\n",
        "  topic_info= topic_model.get_topic_info()\n",
        "  merged_df = pd.merge(news_text_df, topic_info, left_on='topic_label', right_on='Topic', how='left')\n",
        "  # add the 'Value' column from df2 to df1\n",
        "  news_text_df['topic_name'] = merged_df['Name']\n",
        "\n",
        "  #check if previous news is already present or not\n",
        "  if 'link' in news_text_df_old.columns:\n",
        "    news_text_df = news_text_df[~news_text_df['link'].isin(news_text_df_old['link'])]\n",
        "\n",
        "\n",
        "  if news_text_df.empty:\n",
        "    return JSONResponse(content={\"message\": \"No new data to insert\"})\n",
        "\n",
        "  else:\n",
        "\n",
        "\n",
        "    data = news_text_df.to_dict(orient='records')\n",
        "\n",
        "    # insert the data into the MongoDB collection\n",
        "    res = mycol.insert_many(data)\n",
        "\n",
        "    if res.acknowledged:\n",
        "        return JSONResponse(content={\"message\": \"Insertion successful.\"})\n",
        "    else:\n",
        "        return JSONResponse(content={\"message\": \"Insertion failed.\"})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3MNo_e59h6",
        "outputId": "87000712-a293-4cb7-9f5b-8509407b249d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-02T08:22:51+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL:  https://cb6b-34-73-229-95.ngrok.io\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [183]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     120.89.104.44:0 - \"GET /predictandpost HTTP/1.1\" 200 OK\n",
            "INFO:     120.89.104.44:0 - \"GET /predictandpost HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL: \", ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app,port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErmvjC9N8WpK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}